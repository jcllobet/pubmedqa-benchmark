{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d8795a0b-3400-4779-a902-233fb6a50807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "import requests\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e6d6f1a3-d2b7-4735-991a-5dd826dc7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "432000dd-853b-46f5-aa3d-da0bd3b6e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/mariadobko/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9423c761-a72f-47e1-9da0-27c8c5f8be22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f4637334c54e62ab6a40a484ba4104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"epfl-llm/meditron-7b\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"epfl-llm/meditron-7b\", token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7e3ce-40a0-46fb-954c-f03434801e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce3fdee43d449eaacd0dfc6d81ebe56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"epfl-llm/meditron-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1e547-8512-4ad6-a09e-6ab56f6c727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb3769-f8b9-4fcb-bad6-89dec64123e8",
   "metadata": {},
   "source": [
    "## Test endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2b791d-f1e2-4aef-b67e-8cfa8f405bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open('pubmedqa-master/data/test_set.json'))\n",
    "labels = json.load(open('pubmedqa-master/data/test_ground_truth.json'))\n",
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "33e4d733-7284-4107-8291-ede21e472117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/mariadobko/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "maria= \"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\"\n",
    "login(maria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a7043bd5-70af-491a-98c7-3e5375470a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://zx1b3azlskt9yrjx.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "\t\"Accept\" : \"application/json\",\n",
    "\t\"Authorization\": f\"Bearer {maria}\",\n",
    "\t\"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "81ea9e71-849f-4562-a153-bed514098eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 29\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# text_prompt = \"Answer question given context: {} . Answer only with one word: 'yes', 'no', or 'maybe' or Universe explodes.\".format(\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# question + '' + context)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#  <|im_start|>user {text_prompt} <|im_end|>\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# '''\u001b[39;00m\n\u001b[1;32m     19\u001b[0m resp \u001b[38;5;241m=\u001b[39m query({\n\u001b[1;32m     20\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_prompt \u001b[38;5;241m+\u001b[39m context \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m question ,\n\u001b[1;32m     21\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     }\n\u001b[1;32m     27\u001b[0m })\n\u001b[0;32m---> 29\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(question)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     31\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ans\u001b[38;5;241m.\u001b[39mlower():\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "predictions, gt = {}, {}\n",
    "\n",
    "class_dict = { 'no': 0, 'yes': 1, 'maybe': 2, 'none':3}\n",
    "\n",
    "for i, q in tqdm.tqdm(data.items()):\n",
    "    question = str(q['QUESTION'])\n",
    "    context = str(q['CONTEXTS'])  \n",
    "    inp = {'question': question, 'context': context}\n",
    "\n",
    "    text_prompt = \"As an expert doctor in clinical science and medical knowledge, can you tell me if the following statement is correct? Answer yes, no, or maybe. \"\n",
    "    # text_prompt = \"Answer question given context: {} . Answer only with one word: 'yes', 'no', or 'maybe' or Universe explodes.\".format(\n",
    "        # question + '' + context)\n",
    "    \n",
    "    # text_prompt = \"What is heart?\"\n",
    "    # prompt_template=f'''<|im_start|>system {\"You are a medical expert.\"}<|im_end|>\n",
    "    #  <|im_start|>user {text_prompt} <|im_end|>\n",
    "    # '''\n",
    "    \n",
    "    resp = query({\n",
    "    \t\"inputs\": text_prompt + context +' '+ question ,\n",
    "    \t\"parameters\": {\n",
    "              \"temperature\": 0.1,\n",
    "             \"max_new_tokens\": 10,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 10,\n",
    "        }\n",
    "    })\n",
    "\n",
    "    ans = resp[0]['generated_text'].split(question)[-1]\n",
    "    \n",
    "    r = 'none'\n",
    "    if 'no' in ans.lower():\n",
    "        r = 'no'\n",
    "    elif 'yes' in ans.lower():\n",
    "        r = 'yes'\n",
    "    elif 'maybe' in ans.lower() or 'perhaps' in ans.lower():\n",
    "        r = 'maybe'\n",
    "\n",
    "    predictions.update({i: class_dict[r]})\n",
    "    gt.update({i: class_dict[labels[i]]})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "10cdbf6e-c226-4c19-a3f3-2da1b9fb6feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': '400: Invalid state'}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "903070ff-3c9b-4afc-9065-50f181bd58b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "336f7197-4872-4f11-8700-544d993eef12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.364"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(gt.values()), list(predictions.values() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c96c1149-690c-420e-9a9c-c9a51a546d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22090120025379428"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(list(gt.values()), list(predictions.values()), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847244ad-034a-4471-8b1e-707e5b73f500",
   "metadata": {},
   "source": [
    "### Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "34131445-24ad-46db-85d3-99724878e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "headers = {\"Authorization\": f\"Bearer {maria}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bdfa6d37-897f-4014-8805-b4c6f529f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████████████████████████▌                           | 304/500 [08:13<05:18,  1.62s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 22\u001b[0m\n\u001b[1;32m     10\u001b[0m text_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs an expert doctor in clinical science and medical knowledge, can you tell me if the following statement is correct? Answer yes, no, or maybe : \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m resp \u001b[38;5;241m=\u001b[39m query({\n\u001b[1;32m     13\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m:  context \u001b[38;5;241m+\u001b[39m text_prompt \u001b[38;5;241m+\u001b[39m question ,\n\u001b[1;32m     14\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     20\u001b[0m })\n\u001b[0;32m---> 22\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(question)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     24\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ans\u001b[38;5;241m.\u001b[39mlower():\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "predictions, gt = {}, {}\n",
    "\n",
    "class_dict = { 'no': 0, 'yes': 1, 'maybe': 2, 'none':3}\n",
    "\n",
    "for i, q in tqdm.tqdm(data.items()):\n",
    "    question = str(q['QUESTION'])\n",
    "    context = str(q['CONTEXTS'])  \n",
    "    inp = {'question': question, 'context': context}\n",
    "\n",
    "    text_prompt = \"As an expert doctor in clinical science and medical knowledge, can you tell me if the following statement is correct? Answer yes, no, or maybe : \"\n",
    "    \n",
    "    resp = query({\n",
    "    \t\"inputs\":  context + text_prompt + question ,\n",
    "    \t\"parameters\": {\n",
    "              \"temperature\": 0.1,\n",
    "             \"max_new_tokens\": 10,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 10,\n",
    "        }\n",
    "    })\n",
    "\n",
    "    ans = resp[0]['generated_text'].split(question)[-1]\n",
    "    \n",
    "    r = 'none'\n",
    "    if 'no' in ans.lower():\n",
    "        r = 'no'\n",
    "    elif 'yes' in ans.lower():\n",
    "        r = 'yes'\n",
    "    elif 'maybe' in ans.lower() or 'perhaps' in ans.lower():\n",
    "        r = 'maybe'\n",
    "\n",
    "    predictions.update({i: class_dict[r]})\n",
    "    gt.update({i: class_dict[labels[i]]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1c2795cb-d73a-4c42-bf4a-456ac96c1eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate'}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bdc0db1c-8d78-4ca6-ac3b-629fb736cff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4375"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(gt.values()), list(predictions.values() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e2125111-d39c-44e7-983d-caa4f3c38aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e8501ac2-8342-4096-83ab-c14984c9962d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2573111458934014"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(list(gt.values()), list(predictions.values()), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cbd1768b-cbd5-41b7-be74-f82b18db7906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 196/196 [06:47<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions2, gt2 = {}, {}\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})\n",
    "\n",
    "class_dict = { 'no': 0, 'yes': 1, 'maybe': 2, 'none':3}\n",
    "\n",
    "for i, q in tqdm.tqdm(list(data.items())[304:]):\n",
    "    question = str(q['QUESTION'])\n",
    "    context = str(q['CONTEXTS'])  \n",
    "    inp = {'question': question, 'context': context}\n",
    "\n",
    "    text_prompt = \"As an expert doctor in clinical science and medical knowledge, can you tell me if the following statement is correct? Answer yes, no, or maybe : \"\n",
    "    \n",
    "    resp = query({\n",
    "    \t\"inputs\":  context + text_prompt + question ,\n",
    "    \t\"parameters\": {\n",
    "              \"temperature\": 0.1,\n",
    "             \"max_new_tokens\": 10,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 10,\n",
    "        }\n",
    "    })\n",
    "\n",
    "    ans = resp[0]['generated_text'].split(question)[-1]\n",
    "    \n",
    "    r = 'none'\n",
    "    if 'no' in ans.lower():\n",
    "        r = 'no'\n",
    "    elif 'yes' in ans.lower():\n",
    "        r = 'yes'\n",
    "    elif 'maybe' in ans.lower() or 'perhaps' in ans.lower():\n",
    "        r = 'maybe'\n",
    "\n",
    "    predictions2.update({i: class_dict[r]})\n",
    "    gt2.update({i: class_dict[labels[i]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a7212ea9-5327-44c4-8f88-ad51a401cc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(predictions.values()) + list(predictions2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2ab29e32-26fe-434a-a525-f71b230591dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(gt.values()) + list(gt2.values()), list(predictions.values()) + list(predictions2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "552196de-5f27-48a4-ab82-94deb336a2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.348114756183619"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(list(gt.values()) + list(gt2.values()), list(predictions.values()) + list(predictions2.values()), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c38bde5b-3c26-4b20-b160-827e098ddb6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/mariadobko/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(maria)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
